{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a610df-6dc6-432a-ad5c-502d992f6c22",
   "metadata": {},
   "source": [
    "# Document question answering\n",
    "\n",
    "This tutorial shows how to build a simple document question-answering system from scratch using OpenAI, [PyMuPDF](https://github.com/pymupdf/PyMuPDF),  and [LanceDB](https://github.com/lancedb/lancedb).\n",
    "\n",
    "Supporting packge (`aiutils`) [is available here.](https://github.com/ploomber/doc/tree/main/aiutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5a2206-0a9b-49a9-8711-bd8f6c17a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "aiutils_cache_logger = logging.getLogger('aiutils.cache')\n",
    "aiutils_cache_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c27bb3-d225-4029-9f8f-ceade527a123",
   "metadata": {},
   "source": [
    "Let's download the [OLMo](https://arxiv.org/abs/2402.00838) paper in PDF format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad31bcff-bb54-4f69-9099-c911b6f9a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2402.00838.pdf\"\n",
    "\n",
    "path_to_data = Path(\".data\")\n",
    "path_to_data.mkdir(exist_ok=True)\n",
    "path_to_paper = path_to_data / \"paper.pdf\"\n",
    "\n",
    "if not path_to_paper.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(path_to_paper, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94902fcb-31c5-439c-9458-e8681c576359",
   "metadata": {},
   "source": [
    "`Document` is a little abstraction to run OCR on `.pdf` files (to extract text), it uses [`PyMuPDF`](https://github.com/pymupdf/PyMuPDF) under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ad533d-b184-4289-a585-09131ce3a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiutils.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a917772-d516-4c86-8537-7cf733b94b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(path=.data/paper.pdf, n_tokens=20,910, price=0.01 USD)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\".data/paper.pdf\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3629e27-4894-478f-a26d-143a1c8fa103",
   "metadata": {},
   "source": [
    "`APICache` caches calls to OpenAI's API in a SQLite database, so calls with the same arguments return the cached response. This allows me to refactor and re-run code without worrying about paying for redundant requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240f5973-161b-477f-8894-3b87c522a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiutils.cache import APICache\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "embeddings_create = APICache(client.embeddings.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9e64e-1487-4a6e-a2a5-4cb72d22b71c",
   "metadata": {},
   "source": [
    "To get the most relevant pages to answer a question, we need to perform vector search. LanceDB is an embedded vector database, it requires no setup so it's perfect for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732a31f6-5c40-47e4-98c1-ba3d1eb6374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "\n",
    "path_to_vector_db = path_to_data / \"vector-db\"\n",
    "\n",
    "if path_to_vector_db.exists():\n",
    "    shutil.rmtree(path_to_vector_db)\n",
    "\n",
    "db = lancedb.connect(path_to_vector_db)\n",
    "\n",
    "# vector contains the embeddings we'll compute for each page\n",
    "# content contains the page's text\n",
    "schema = pa.schema([pa.field(\"vector\", pa.list_(pa.float32(), list_size=1536)),\n",
    "                    pa.field(\"content\", pa.string())])\n",
    "table = db.create_table(\"embeddings\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cd3c5-a33b-4f76-a5e4-67eed8d96ae3",
   "metadata": {},
   "source": [
    "Iterate over pages, compute the embedding and insert it into the db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b87f9d6-7e56-4815-82af-1c71c79a5f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n",
      "INFO:aiutils.cache:Cache hit, using cached response.\n"
     ]
    }
   ],
   "source": [
    "for page in doc.pages():\n",
    "    response = embeddings_create(\n",
    "    input=page,\n",
    "    model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    embedding = response.data[0].embedding\n",
    "    data = dict(vector=embedding, content=page)\n",
    "    table.add(data=[data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670566f-f01f-4f36-aec4-8aefb40def61",
   "metadata": {},
   "source": [
    "We have everything we need, let's put the user's query in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a904258-4f71-41e7-8171-c29da115ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Which optimizer was used to train this model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ac88c-0567-4661-944f-1f40c250483c",
   "metadata": {},
   "source": [
    "We need to compute the embedding for the user's query, so we can search for the most relevent pages in the doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe435ec-895b-45ef-9b6b-359347d06851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiutils.cache:Cache hit, using cached response.\n"
     ]
    }
   ],
   "source": [
    "response = embeddings_create(\n",
    "    input=user_query,\n",
    "    model=\"text-embedding-3-small\")\n",
    "\n",
    "embedding_query = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f074b-3788-48b9-b061-190fa6998c4a",
   "metadata": {},
   "source": [
    "We search for the most relevant pages using the vector DB and retrieve the text from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21249e2d-452b-4912-9131-0508a79206e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = table.search(embedding_query).limit(2)\n",
    "content = [r[\"content\"] for r in result.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa03518-c224-4217-bd55-c3595b534e32",
   "metadata": {},
   "source": [
    "We convert the retrieved pages into a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c90591b4-0718-4e6c-b17a-5bd586eb345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "template = Template(\"\"\"\n",
    "{% for string in strings %}\n",
    "### START PAGE ###\n",
    "{{ string }}\n",
    "{% endfor %}\n",
    "\"\"\")\n",
    "\n",
    "rendered = template.render(strings=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cc041-f2cf-4f68-a9b5-d432da12cecf",
   "metadata": {},
   "source": [
    "Now we use OpenAI to answer the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4d755d1-d901-4d7e-b05c-4cf94ee79778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiutils.cache:Cache hit, using cached response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which optimizer was used to train this model?\n",
      "\n",
      "\n",
      "The AdamW optimizer was used to train this model.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You're a system that answers questions from a document.\n",
    "\n",
    "Here are the relevant sections from the document, use it to answer the question. Each\n",
    "page is separated by: ### START PAGE ###\n",
    "\n",
    "{rendered}\n",
    "\"\"\"\n",
    "\n",
    "completions_create = APICache(client.chat.completions.create)\n",
    "\n",
    "response = completions_create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ])\n",
    "\n",
    "print(user_query + '\\n\\n')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d08f7-69ae-4e16-b2dc-b37e693f2227",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The answer is correct! In section \"3.2 Optimizer\", the paper says:\n",
    "\n",
    "> We use the AdamW optimizer (Loshchilov and Hutter, 2019) with the hyperparameters shown in\n",
    "Table 4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
